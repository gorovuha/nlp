{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1z6h68tbs9gdTKHQiadK-WzUPHIVGNXB6",
      "authorship_tag": "ABX9TyMsoNL04R904soZeDIkaIRo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gorovuha/nlp/blob/main/rake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymorphy2"
      ],
      "metadata": {
        "id": "wF2JmYhdp9ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "import io\n",
        "import os\n",
        "import pymorphy2 as pm\n",
        "import codecs\n",
        "import string\n",
        "import operator\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU1o6FD-UWzu",
        "outputId": "ff279887-d188-4a94-c177-3e8d448266f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = pm.MorphAnalyzer()\n",
        "\n",
        "cache = {}\n",
        "\n",
        "def mparse(w):\n",
        "  if w not in cache.keys():\n",
        "    cache[w] = m.parse(w)\n",
        "  return cache[w]\n",
        "\n",
        "count = 1\n",
        "\n",
        "for dirpath, _, filenames in os.walk('/content/drive/MyDrive/in'):\n",
        "  for filename in filenames:\n",
        "    words = []\n",
        "    words2 = \"\"\n",
        "    direction = '/content/drive/MyDrive/out/'\n",
        "    name = 'chuncked_'\n",
        "    with io.open(os.path.join('/content/drive/MyDrive/in', filename), 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "        words = text.split()\n",
        "        print(len(words))\n",
        "        for i in range(len(words)-1):\n",
        "            if mparse(words[i])[0].tag.POS == \"ADVB\" or mparse(words[i])[0].tag.POS == \"VERB\" or \\\n",
        "                    mparse(words[i][:-1])[0].tag.POS == \"VERB\" or mparse(words[i])[0].tag.POS == \"PRTS\" or \\\n",
        "                    mparse(words[i])[0].tag.POS == \"INFN\" or mparse(words[i])[0].tag.POS == \"COMP\" or \\\n",
        "                    mparse(words[i])[0].tag.POS == \"ADJS\" or mparse(words[i])[0].tag.POS == \"GRND\" or \\\n",
        "                    mparse(words[i])[0].tag.POS == \"CONJ\":\n",
        "                words2 += \" | \" + words[i] + \" | \"\n",
        "            elif (mparse(words[i])[0].tag.POS == \"NOUN\" and mparse(words[i])[0].tag.case == (\"gent\" or \"accs\")):\n",
        "                words2 += words[i] + \" | \"\n",
        "            elif mparse(words[i])[0].tag.POS == \"NOUN\" and mparse(words[i + 1])[0].tag.POS == (\"NOUN\" or \"ADJF\") and \\\n",
        "                    mparse(words[i])[0].tag.case != mparse(words[i + 1])[0].tag.case and mparse(words[i + 1])[\n",
        "                0].tag.case != \"gent\":\n",
        "                words2 += words[i] + \" | \"\n",
        "            elif (mparse(words[i])[0].tag.POS == \"NOUN\" and mparse(words[i])[0].tag.case != (\"nomn\" or \"accs\") and\n",
        "                  mparse(words[i + 1])[0].tag.POS == \"NOUN\" and mparse(words[i + 1])[0].tag.case == (\n",
        "                          \"nomn\" or \"accs\")):\n",
        "                words2 += words[i] + \" | \"\n",
        "            elif (mparse(words[i])[0].tag.POS == None and mparse(words[i][:-1])[0].tag.POS == None and\n",
        "                  mparse(words[i][1:])[0].tag.POS == None):\n",
        "                words2 += \" | \" + words[i] + \" | \"\n",
        "            elif (mparse(words[i])[0].tag.POS == \"NOUN\" and mparse(words[i - 1])[0].tag.POS == \"ADJF\" or\n",
        "                  mparse(words[i - 1])[0].tag.POS == \"PRTF\") and mparse(words[i])[0].tag.case == \\\n",
        "                    mparse(words[i + 1])[0].tag.case:\n",
        "                words2 += words[i] + \" | \"\n",
        "            else:\n",
        "                words2 += words[i] + \" \"\n",
        "            if i%1000==0:\n",
        "               print(str(i/len(words)*100))\n",
        "    new_file = io.open(direction + name + str(count) + '.txt', 'w', encoding='utf-8')\n",
        "    new_file.write(words2)\n",
        "    new_file.close()\n",
        "    count += 1\n",
        "    print(count)"
      ],
      "metadata": {
        "id": "t8B4QaTzUhLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punkt = open('/content/drive/MyDrive/stopwords/punct.txt', 'r', encoding='utf-8').read()\n",
        "print(type(punkt))\n",
        "stop = open('/content/drive/MyDrive/stopwords/stop.txt', 'r', encoding='utf-8').read()\n",
        "pronouns = open('/content/drive/MyDrive/stopwords/pronouns.txt', 'r', encoding='utf-8').read()\n",
        "preps = open('/content/drive/MyDrive/stopwords/preps.txt', 'r', encoding='utf-8').read()\n",
        "mystop = open('/content/drive/MyDrive/stopwords/userstop.txt', 'r', encoding='utf-8').read()\n",
        "\n",
        "punkt_list = nltk.word_tokenize(punkt)\n",
        "stop_list = nltk.word_tokenize(stop) + nltk.word_tokenize(pronouns) + nltk.word_tokenize(preps) + nltk.word_tokenize(mystop)\n"
      ],
      "metadata": {
        "id": "iOtxiTsCUsmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def isPunct(word):\n",
        "    return len(word) == 1 and (word in string.punctuation or word in punkt_list)\n",
        "\n",
        "def isNumeric(word):\n",
        "    try:\n",
        "        float(word) if '.' in word else int(word)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def isInitial(word):\n",
        "    return len(word) == 2 and word[1] == '.'"
      ],
      "metadata": {
        "id": "vndv0KVdUv8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7mF6pz8cM6n"
      },
      "outputs": [],
      "source": [
        "class RakeKeywordExtractor:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stopwords = set(nltk.corpus.stopwords.words())\n",
        "        self.top_fraction = 1\n",
        "\n",
        "    def _generate_candidate_keywords(self, sentences):\n",
        "        phrase_list = []\n",
        "        for sentence in sentences:\n",
        "            words = map(lambda x: '|' if x in self.stopwords or x in stop_list or isNumeric(x) or isInitial(x) else x, \\\n",
        "                        nltk.word_tokenize(sentence.lower()))\n",
        "            phrase = []\n",
        "            for word in words:\n",
        "                if word == '|' or isPunct(word):\n",
        "                    if len(phrase) > 0:\n",
        "                        phrase_list.append(phrase)\n",
        "                        phrase = []\n",
        "                else:\n",
        "                    phrase.append(word)\n",
        "        return phrase_list\n",
        "\n",
        "    def _calculate_word_scores(self, phrase_list):\n",
        "        word_freq = nltk.FreqDist()\n",
        "        word_degree = nltk.FreqDist()\n",
        "        for phrase in phrase_list:\n",
        "            degree = len(list(filter(lambda x: not isNumeric(x), phrase))) - 1\n",
        "            for word in phrase:\n",
        "                word_freq[word] += 1\n",
        "                word_degree[word] += degree\n",
        "        word_scores = {}\n",
        "        for word in word_freq.keys():\n",
        "            word_scores[word] = word_degree[word] / word_freq[word]\n",
        "        return word_scores\n",
        "\n",
        "    def _calculate_phrase_scores(self, phrase_list, word_scores):\n",
        "        phrase_scores = {}\n",
        "        for phrase in phrase_list:\n",
        "            phrase_score = 0\n",
        "            for word in phrase:\n",
        "                phrase_score += word_scores[word]\n",
        "            phrase_scores[' '.join(phrase)] = phrase_score\n",
        "        return phrase_scores\n",
        "\n",
        "    def extract(self, text, incl_scores = False):\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        print('sentences done')\n",
        "        phrase_list = self._generate_candidate_keywords(sentences)\n",
        "        print('phrases done')\n",
        "        word_scores = self._calculate_word_scores(phrase_list)\n",
        "        print('words done')\n",
        "        phrase_scores = self._calculate_phrase_scores(phrase_list, word_scores)\n",
        "        print('scores done')\n",
        "        sorted_phrase_scores = sorted(phrase_scores.items(), key = operator.itemgetter(1), reverse = True)\n",
        "        n_phrases = len(sorted_phrase_scores)\n",
        "        if incl_scores:\n",
        "            return sorted_phrase_scores[0 : int(n_phrases / self.top_fraction)]\n",
        "        else:\n",
        "            return map(lambda x: x[0], sorted_phrase_scores[0 : int(n_phrases / self.top_fraction)])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rake = RakeKeywordExtractor()\n",
        "count = 1\n",
        "\n",
        "for dirpath, _, filenames in os.walk('/content/drive/MyDrive/out'):\n",
        "  for filename in filenames:\n",
        "    with io.open(os.path.join('/content/drive/MyDrive/out/', filename), 'r', encoding='utf-8') as f:\n",
        "        txt = f.read()\n",
        "\n",
        "    keywords = rake.extract(txt, incl_scores=True)\n",
        "    direction = '/content/drive/MyDrive/keywords/'\n",
        "    name = 'keywords_from_'\n",
        "    new_file = io.open(direction + name + str(count) + '.txt', 'w', encoding='utf-8')\n",
        "    for eachkwrd in keywords:\n",
        "        new_file.write(str(eachkwrd[0]) + \";\" + str(eachkwrd[1]) + '\\n')\n",
        "    count += 1\n",
        "    new_file.close()\n",
        "    print(count)"
      ],
      "metadata": {
        "id": "6iZq9wifUVCb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}